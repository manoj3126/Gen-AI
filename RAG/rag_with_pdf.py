# -*- coding: utf-8 -*-
"""RAG with pdf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q78zMQMeuzdAb_w2jGm8oDgRCvpI1uBO

ðŸ”¹ What is RAG?
Normlly, LLMs(like GPT) only know what they were trained on.
RAG connect them with your own data (like pdfs, notes, databases)
workflow:
1) Extract Text from pdf
2) convert text into Embeddings(numerical vectors)
3) stores embeddings in a vector databases (FAISS, pipecone)
4) when you ask a question--- search the DB--- give relevant text-- LLM generates the answers
"""

# Install required Libraries
!pip install pypdf2 faiss-cpu sentence-transformers transformers

# Extract text from pdf
from PyPDF2 import PdfReader

# load pdf
reader=PdfReader('/content/sample.pdf')
text = ""
for page in reader.pages:
  text+=page.extract_text()

print(text[:500]) # shows first 500 chars

# create embeddings
from sentence_transformers import SentenceTransformer

# Use pre-trained embedding model
embedder= SentenceTransformer('all-MiniLM-L6-v2')

# split text into chunks
chunks= text.split(". ")
embeddings= embedder.encode(chunks)

print("Number of chunks:",len(chunks))

# Stores in FAISS (Vector database)
import faiss
import numpy as np

# convert to FAISS index
dimension= embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

# Query with a Question
query = 'What is Generative AI?'
query_embedding= embedder.encode([query])

# search top 3 relevant chunks
D, I = index.search(np.array(query_embedding),k=3)
print("\nðŸ”¹ Retrieved Chunks:")
for idx in I[0]:
  print(chunks[idx])

# pass Retrived chunks to LLM
from transformers import pipeline

generator= pipeline('text-generation',model='gpt2')

context= " ".join([chunks[i] for i in I[0]])
prompt = f"Anwers the question using the context:\nContext: {context}\nQuestion: {query}\nAnwer:"

result= generator(prompt,
                  max_length=50,
                  temperature=0.7,
                  do_sample=True)
print("\nFinal Answer:",result[0]['generated_text'])