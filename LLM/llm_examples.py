# -*- coding: utf-8 -*-
"""LLM-Examples.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WkXpg6VZu97hHNRkTKeOt9ecxo_KvNeE

Text Generation
"""

# import libranry
from transformers import pipeline

# load a text generation pre-trained model
generator = pipeline('text-generation',model='gpt2')

# generate text
result= generator(
    "Once upon a time in AI,",
    max_length=20,
    num_return_sequences=1
)
# print generated text
print(result[0]['generated_text'])

"""üìù Mini Exercise: Text Generation

üëâ Task: Use GPT-2 to generate 2 different creative story continuations for the sentence:
‚ÄúIn 2050, humans and AI‚Ä¶‚Äù

steps:
1. load pipeline
2. set max_length=50
3. Use num_return_sequence=2
4. Add temperature=0.9 and top_p=0.9 for creativity
"""

from transformers import pipeline

# Load GPT-2 text generation pipeline
generator = pipeline('text-generation',model='gpt2')

# prompt
prompt = "In 2025, human and AI"

# Generate 2 creative continuations
results = generator(prompt,
                   max_length=60,
                   num_return_sequences=2,
                   temperature=0.9,
                   top_p=0.9,
                   do_sample=True
)
# print outputs
for i,res in enumerate(results):
  print(f"\n---Story{i+1}----\n")
  print(res['generated_text'])

"""üî• Temperature Effect in Text Generation"""

from transformers import pipeline

# Load GPT-2 text generation pipeline
generator = pipeline('text-generation',model='gpt2')

# prompt
prompt= 'In 2025, human and AI'

# Low temperature (predictable, less creative)
low_temp= generator(prompt,
                    max_length=50,
                    temperature=0.2,
                    do_sample=True)

# high temperature (creative, random)
high_temp= generator(prompt,
                     max_length=50,
                     temperature=1.2,
                     do_sample=True)

# print the outputs
print("\n---Low Temperature (0.2)---\n")
print(low_temp[0]['generated_text'])

print("\n---High Temperature (1.2)---\n")
print(high_temp[0]['generated_text'])

"""Summarization"""

# import and load model
# we will use T5-small good for summarization
from transformers import pipeline

# Load summarization pipeline
summarizer = pipeline('summarization',model='t5-small')

# provide long text
text = """
Generative AI is a type of artificial intelligence that can create new content,
such as text, images, music, or even videos. It works by learning from large
amounts of existing data and then generating new outputs that resemble the
original data. Applications of generative AI include chatbots, content creation,
drug discovery, and personalized recommendations. However, it also raises
ethical concerns such as bias, misinformation, and job displacement.
"""
#summarize
summary= summarizer(text,
                    max_length=40,
                    min_length=10,
                    do_sample=False)
# print summary
print("Summary:",summary[0]['summary_text'])

"""üìù Mini Exercise: Summarization

üëâ Task: Summarize the following text into 1‚Äì2 sentences.

Artificial Intelligence is rapidly transforming industries worldwide.
It helps businesses automate processes, improve customer experiences,
and make data-driven decisions. AI also powers technologies like self-driving cars,
virtual assistants, fraud detection systems, and medical diagnosis tools.
Despite its benefits, AI poses challenges such as ethical concerns,
job displacement, and potential misuse of data.
"""

from transformers import pipeline

# Load summarizer
summarizer = pipeline('summarization',model='t5-small')

# Input text
text="""Artificial Intelligence is rapidly transforming industries worldwide.
It helps businesses automate processes, improve customer experiences,
and make data-driven decisions. AI also powers technologies like self-driving cars,
virtual assistants, fraud detection systems, and medical diagnosis tools.
Despite its benefits, AI poses challenges such as ethical concerns,
job displacement, and potential misuse of data.
"""
# summarize
summary = summarizer(text,
                     max_length=40,
                     min_length=10,
                     do_sample=False)
# summary
print('summary:',summary[0]['summary_text'])

"""Sentiment Analysis"""

# This is one of the most common NLP classification tasks. It tells whether text is positive, negative, or neutral.

# Load Sentiment pipeline
from transformers import pipeline

# load sentiment analysis pipeline
classifier = pipeline('sentiment-analysis')

# Test on some examples
print(classifier('I love learning Generative AI!'))
print(classifier('This project is very boring.'))
print(classifier('The result are okay, nothing special.'))

"""üîπ Mini Exercise for You

üëâ Classify the sentiment of these 3 sentences:

"The new AI tool saves me hours of work!"

"I am worried about AI taking away jobs."

"AI has both advantages and disadvantages."
"""

from transformers import pipeline

# load sentiment analysis pipeline
classifier= pipeline('sentiment-analysis')

# Test above sentence
print(classifier('The new AI tool saves me, hours of work!'))
print(classifier('I am worried about AI taking away jobs.'))
print(classifier('AI has both advantages and disadvantages.'))

"""Image Generation"""

# Image Generation with Stable Diffusion

# install required libraries
# !pip install diffusers transformers accelerate safetensors

# import and load stable Diffusion
from diffusers import StableDiffusionPipeline
import torch

# load stable diffusion model
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")

# check to GPU is available
pipe= pipe.to('cuda' if torch.cuda.is_available() else 'cpu')

# Generate an Image
prompt = 'a futuristic city skyline at sunset, digital art'
image = pipe(prompt).images[0]

# show image
image.show()

# save the image
image.save('future_city.png')

"""üîπ Mini Exercise for You

Try generating an image for this prompt:
üëâ "a cozy library with warm lights, filled with ancient books, in Studio Ghibli style"
"""

from diffusers import StableDiffusionPipeline
import torch

# load a stable diffusion pipleine
pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')

# check GPU is available
pipe = pipe.to('cuda' if torch.cuda.is_available() else 'cpu')

# generate image
prompt = 'a cozy library with warm lights, filled with ancient books, in Studio Ghibli style'
image= pipe(prompt).images[0]

# show image
image.show()

# save image
image.save('library.png')

"""üñº Generating Multiple Images in One Go"""

from diffusers import StableDiffusionPipeline
import torch

# load a stable diffusion pipeline
pipe= StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')

# check GPU is present
pipe= pipe.to('cuda' if torch.cuda.is_available() else 'cpu')

# generate image
prompt = "a magical forest with glowing mushrooms, fantasy art"
image = pipe(prompt,num_images_per_prompt=3).images

# display and save
for i, img in enumerate(images):
  img.show()
  img.save(f"forest_image_{i+1}.png")

