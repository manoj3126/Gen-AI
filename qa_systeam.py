# -*- coding: utf-8 -*-
"""qa-systeam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SbWKMKqjSMyCbJ-69bWsT4xBT-Zgo6z6

Step 1: Problem Understanding

We want a chatbot that:

Accepts user queries (e.g., "What is the refund policy?")

Retrieves relevant answers from company FAQs or policy docs

Uses a Retrieval-Augmented Generation (RAG) pipeline for accuracy

Step 2: Components Needed

Document Store (Vector DB) – to store and retrieve embeddings (FAISS)

Embedding Model – to convert text into vectors (HuggingFace sentence-transformers)

Retriever – to fetch top-k relevant chunks from the database

LLM (Generator) – to generate a final response using retrieved context (HuggingFace model or OpenAI GPT)

Framework – LangChain for pipeline orchestration

Interface – Streamlit for chat UI

Step 3: Workflow

Data Collection: Gather company FAQs, policies → Convert to text

Chunking: Split documents into small chunks (e.g., 500 tokens)

Embedding: Generate vector representations for each chunk

Store in FAISS: Save embeddings for fast similarity search

Query Processing:

User enters query

Embed query and search in FAISS

Retrieve top-k relevant chunks

Generate Answer:

Pass query + retrieved chunks to LLM

Generate final response

Return Answer to user
"""

### Practical Implementation

# Install Dependencies
# !pip install langchain faiss-cpu sentence-transformers transformers streamlit

# prepare documents
from langchain.text_splitter import RecursiveCharacterTextSplitter

# sample FAQ Text
faq_text = """
Q: What is the refund policy?
A: You can request a refund within 30 days of purchase.

Q: How can I contact support?
A: You can email us at support@example.com.

Q: Is there a warranty on products?
A: Yes, all products come with a 1-year warranty.
"""

# split text into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=20
)
docs = text_splitter.split_text(faq_text)

# Create Embeddings and Store in FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# use sentence-transformers model
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# create FAISS vector stores
vectorstores= FAISS.from_texts(docs,embedding_model)

# create a retriever
retriever = vectorstores.as_retriever(search_type='similarity',search_kwargs={'k':2})

# Load LLM
from langchain.llms import HuggingFacePipeline

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.llms import HuggingFacePipeline

model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_length=128)
llm = HuggingFacePipeline(pipeline=pipe)

# create a qa chain
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type= 'stuff',
    return_source_documents=False
)

# Test the systeam
query = "What is your refund policy?"
response = qa_chain.run(query)
print("User:", query)
print("Bot:", response)

# Deploy
# Use Streamlit for UI
import streamlit as st

st.title('Customer Support Chatbot')
user_input = st.text_input('Ask your question:')

if st.button("Get Answer"):
  response = qa_chain.run(user_input)
  st.write(response)



